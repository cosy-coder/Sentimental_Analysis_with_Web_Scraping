# -*- coding: utf-8 -*-
"""test.ipynb


// it is highly recommended to go to google collab to run the program for hastle free execution ::: 
// Also before running this make sure to import all the neccessary files such as input excel sheet  , All stopWord files and all the positive and negative words file also.

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wPrPoNE0TBbu8K2IeeD0RVrN5FoYekOU
"""

!pip install requests
!pip install  beautifulsoup4 newspaper3k
!pip install spacy
!python -m spacy download en_core_web_sm
!pip install pandas openpyxl
!pip install syllapy # Install the missing 'syllapy' library.
import csv
import syllapy
from openpyxl import load_workbook
import requests
import re
import string
import spacy
import pandas as pd
from bs4 import BeautifulSoup
from newspaper import Article

def extract_article_stats(particular_url):
  def extract_article(url):
    # Step 1: Fetch the webpage content
    response = requests.get(url)
    response.raise_for_status()  # Ensure we got a valid response

    # Step 2: Use newspaper3k to extract the article
    article = Article(url)
    article.download()
    article.parse()

    # Get the title and text
    title = article.title
    text = article.text

    return title, text

  # Example usage
  url = 'https://insights.blackcoffer.com/rising-it-cities-and-its-impact-on-the-economy-environment-infrastructure-and-city-life-by-the-year-2040-2/'
  title, text = extract_article(url)

  All_text = title + text


  words1 = title.split()  # Split string1 by whitespace
  words2 = text.split()  # Split string2 by whitespace

  Uncleaned_list = words1 + words2

  # print(Uncleaned_list)



  # Example list of file paths (replace with actual paths)
  file_paths = [
      '/StopWords_Names.txt' , '/StopWords_DatesandNumbers.txt' , '/StopWords_Generic.txt' , '/StopWords_Geographic.txt' , '/StopWords_GenericLong.txt'
  ]

  def extract_words_from_file(file_path):
      with open(file_path, 'r') as file:
          content = file.read()  # Read the entire content of the file

      # Remove the '|' character
      content = content.replace('|', '')

      # Split the content into words based on whitespace
      words = content.split()

      return words

  # Step 3: Process each file and collect all words
  all_words = []

  for file_path in file_paths:
      words_list = extract_words_from_file(file_path)
      all_words.extend(words_list)

  # Print all collected words
  # print(all_words)

  def extract_words_from_ANSI(file_path):
      with open(file_path, 'r', encoding='latin-1') as file: # Try opening with Latin-1 encoding
          content = file.read()  # Read the entire content of the file

      # Remove the '|' character
      content = content.replace('|', '')

      # Split the content into words based on whitespace
      words = content.split()

      return words

  # Correct the indentation here
  word_list = (extract_words_from_ANSI('/StopWords_Currencies.txt'))
  all_words.extend(word_list)

  # all_words.remove('.002%.')
  # all_words.remove('www.census.gov.genealogy/names/dist.all.last')

  # print(all_words)

  positive_words_list = extract_words_from_file('/positive-words.txt')
  # Remove all elements from positive list   that are also in stoplist
  positive_words_list = [item for item in positive_words_list if item not in all_words]
  # print(positive_words_list)

  negative_words_list = extract_words_from_ANSI('/negative-words.txt')
  negative_words_list = [item for item in negative_words_list if item not in all_words]
  # print(negative_words_list)

  positive_score = len(positive_words_list)
  negative_score = len(negative_words_list)
  # print(positive_score)
  # print(negative_score)

  polarity_score = (positive_score - negative_score)/(positive_score + negative_score + 0.000001)
  # print(polarity_score)


  def count_sentences_spacy(text):
      nlp = spacy.load("en_core_web_sm")
      doc = nlp(text)
      sentences = list(doc.sents)  # spaCy's sentence segmentation
      return len(sentences)

  no_of_sentences = count_sentences_spacy(All_text)
  # print(no_of_sentences)

  no_of_words = len(Uncleaned_list)
  # print(no_of_words)



  def count_complex_words(text):
      # Split the text into words
      words = text.split()

      # Count words with more than two syllables
      complex_words = [word for word in words if syllapy.count(word) > 2]

      return len(complex_words)

  no_of_complex_words = count_complex_words(All_text)
  percentage_of_complex_words = (no_of_complex_words / no_of_words) * 100
  # print(percentage_of_complex_words)

  Fog_index =  0.4 * (no_of_complex_words / no_of_words)
  # print(Fog_index)

  Average_no_of_words = no_of_words / no_of_sentences
  # print(Average_no_of_words)

  cleaned_list = [item for item in Uncleaned_list if item not in all_words]
  # print(len(cleaned_list))

  subjectivity_score = (positive_score-negative_score)/(no_of_words + 0.000001)
  # print(subjectivity_score)


  def remove_punctuation(text):
      translator = str.maketrans('', '', string.punctuation)

      return text.translate(translator)


  # Remove punctuation from each string in the list
  cleaned_list = [remove_punctuation(string) for string in cleaned_list]


  Word_count = len (cleaned_list)
  # print(Word_count)

  def count_syllables(word):
      vowels = "aeiou"
      num_syllables = 0
      prev_char = ''

      # Check for exceptions like ending with "es" or "ed"
      if word.endswith("es") or word.endswith("ed"):
          return 0

      for char in word.lower():
          if char in vowels and prev_char not in vowels:
              num_syllables += 1
          prev_char = char

      if word.endswith("e"):
          num_syllables -= 1
      if num_syllables == 0 and len(word) > 1:
          num_syllables = 1

      return num_syllables

  def count_syllables_in_text(text):
      words = text.split()
      syllable_counts = [count_syllables(word) for word in words]
      return syllable_counts

  # Count syllables in each word of the text
  syllable_counts = count_syllables_in_text(text)

  #total_syllable_count_per_word::
  syllable_per_word = 0


  # Print the result
  # print("Syllable counts for each word:")
  # for word, syllables in zip(text.split(), syllable_counts):
  #     print(f"{word}: {syllables}")
  #     syllable_per_word += syllables

  # print(syllable_per_word)


  def count_personal_pronouns(text):
      # Define the personal pronouns to count
      pronouns = ["I", "we", "my", "ours", "us" , "We" , "My" , "Ours" , "Us"]

      # Initialize a dictionary to store counts
      pronoun_counts = {pronoun: 0 for pronoun in pronouns}

      # Split text into words and iterate over each word
      words = text.split()
      for word in words:
          # Check if the word matches any of the pronouns (case insensitive)

          if word in pronouns:
              # Increment count for the matching pronoun
              pronoun_counts[word] += 1

      return pronoun_counts


  pronoun_counts = count_personal_pronouns(All_text)

  personal_pronoun_count = 0



  # Print the result
  # print("Personal pronoun counts:")
  # for pronoun, count in pronoun_counts.items():
  #     print(f"{pronoun}: {count}")
  #     personal_pronoun_count += count

  # print(personal_pronoun_count)

  def sum_of_characters(words_list):
      total_sum = 0

      for word in words_list:
          total_sum += len(word)

      return total_sum

  total_no_of_characters = sum_of_characters(Uncleaned_list)
  # print(total_no_of_characters)

  no_of_words = len(Uncleaned_list)
  # print(no_of_words)

  Average_word_length = total_no_of_characters / no_of_words
  Average_sentence_length = total_no_of_characters / no_of_sentences
  # print(Average_word_length)
  # print(Average_sentence_length)

  # print("positive_score::" , positive_score)
  # print("negative_score::" , negative_score)
  # print("polarity_score::" , polarity_score)
  # print("subjectivity_score::" , subjectivity_score)
  # print("Average_sentence_length::" , Average_sentence_length)
  # print("percentage_of_complex_words::" , percentage_of_complex_words)
  # print("Fog_index::" , Fog_index)
  # print("Average_number_of_words::" , Average_no_of_words)
  # print("complex_word_count::" , no_of_complex_words)
  # print("word_count::" , no_of_words)
  # print("syllable_per_word::" , syllable_per_word)
  # print("personal_pronoun_count::" , personal_pronoun_count)
  # print("Average_word_length::" , Average_word_length)

  # output_string = f"{positive_score}, {negative_score}, {polarity_score}, {subjectivity_score}, {Average_sentence_length}, {percentage_of_complex_words}, {Fog_index}, {Average_no_of_words}, {no_of_complex_words}, {no_of_words}, {syllable_per_word}, {personal_pronoun_count}, {Average_word_length}"

  # # Print the combined string
  #   print(output_string)

  values = [
      positive_score, negative_score, polarity_score, subjectivity_score,
      Average_sentence_length, percentage_of_complex_words, Fog_index,
      Average_no_of_words, no_of_complex_words, no_of_words, syllable_per_word,
      personal_pronoun_count, Average_word_length
  ]


  csv_file_path = '/content/Blackcoffer_Assignment.Output.csv'


  # Write the values to the CSV file
  with open(csv_file_path, 'w', newline='') as csvfile:
      writer = csv.writer(csvfile)
      writer.writerow(values)

  print(values)

df = pd.read_excel('/content/Input.xlsx', engine='openpyxl')
urls = df['URL']
for url in urls:
    extract_article_stats(url)  # Or perform any other processing here